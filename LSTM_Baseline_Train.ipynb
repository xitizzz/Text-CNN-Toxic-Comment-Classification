{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation, Bidirectional, InputLayer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyperparam = {'embedding_dim': 300, \n",
    "              'filters': 100, \n",
    "              'kernel_size': 3,\n",
    "              'dropout' : 0.7,\n",
    "              'n_class': 6,\n",
    "              'conv_activation': 'relu', \n",
    "              'dense_activation':'relu',\n",
    "              'batch_size': 128}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wordvectors(words):\n",
    "    try:\n",
    "        embed = word_vec.wv[words]\n",
    "    except ValueError:\n",
    "        embed = None\n",
    "    except KeyError:\n",
    "        words_n = []\n",
    "        for word in words:\n",
    "            if word in word_vec.wv.vocab:\n",
    "                words_n.append(word)\n",
    "        if words_n:\n",
    "            embed = word_vec.wv[words_n]\n",
    "        else:\n",
    "            embed = None\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_zero(arr):\n",
    "    pad_len = max(maxlen - arr.shape[0], 0)\n",
    "    if pad_len > 0:\n",
    "        arr = np.vstack((arr, np.zeros((pad_len, hyperparam['embedding_dim']))))\n",
    "    return arr[:maxlen, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_nan(arr):\n",
    "    pad_len = max(maxlen - arr.shape[0], 0)\n",
    "    if pad_len > 0:\n",
    "        padding = np.empty((pad_len, hyperparam['embedding_dim']), dtype=np.float32)\n",
    "        padding[:] = np.nan\n",
    "        arr = np.vstack((arr, padding))\n",
    "    return arr[:maxlen, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists('./data/GoogleNews-vectors-negative300.bin'):\n",
    "    word_vec = KeyedVectors.load_word2vec_format(fname='./data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "elif os.path.exists('./data/GoogleNews-vectors-negative300.bin.gz'):\n",
    "    google_w2v = gzip.open('./data/GoogleNews-vectors-negative300.bin.gz', 'rb')\n",
    "    word_vec = KeyedVectors.load_word2vec_format(fname=google_w2v, binary=True)\n",
    "else:\n",
    "    print('Embedings not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TokenVectorizer = CountVectorizer(stop_words=set(stopwords.words('english')), lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = TokenVectorizer.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['comment_tokens'] = train['comment_text'].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['comment_wordvec'] = train['comment_tokens'].apply(get_wordvectors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.loc[~train['comment_wordvec'].isnull(), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['wv_len'] = train['comment_wordvec'].apply(lambda x: x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.index = np.arange(train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.empty((train.shape[0], maxlen, hyperparam['embedding_dim']), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, row in train.iterrows():\n",
    "    X_train[i, :, :] = pad_zero(row['comment_wordvec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train['padded_wv'] = train['comment_wordvec'].apply(pad_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train  = np.stack(train['padded_wv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95731, 100, 300)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computation_graph():\n",
    "    model  = Sequential()\n",
    "    model.add(InputLayer((100, hyperparam['embedding_dim'])))\n",
    "    model.add(Bidirectional(LSTM(units=100)))\n",
    "    model.add(Dropout(rate=hyperparam['dropout']))\n",
    "#     model.add(Dense(units=hyperparam['filters']))\n",
    "#     model.add(Dropout(rate=hyperparam['dropout']))\n",
    "#     model.add(Activation(hyperparam['dense_activation']))\n",
    "    model.add(Dense(units=hyperparam['n_class'], activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = computation_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 1206      \n",
      "=================================================================\n",
      "Total params: 322,006\n",
      "Trainable params: 322,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0005), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86157 samples, validate on 9574 samples\n",
      "Epoch 1/10\n",
      "86157/86157 [==============================] - 200s - loss: 0.1076 - acc: 0.9668 - val_loss: 0.0611 - val_acc: 0.9786\n",
      "Epoch 2/10\n",
      "86157/86157 [==============================] - 184s - loss: 0.0614 - acc: 0.9788 - val_loss: 0.0586 - val_acc: 0.9802\n",
      "Epoch 3/10\n",
      "86157/86157 [==============================] - 180s - loss: 0.0564 - acc: 0.9801 - val_loss: 0.0531 - val_acc: 0.9806\n",
      "Epoch 4/10\n",
      "86157/86157 [==============================] - 177s - loss: 0.0540 - acc: 0.9807 - val_loss: 0.0524 - val_acc: 0.9810\n",
      "Epoch 5/10\n",
      "86157/86157 [==============================] - 177s - loss: 0.0527 - acc: 0.9811 - val_loss: 0.0513 - val_acc: 0.9810\n",
      "Epoch 6/10\n",
      "86157/86157 [==============================] - 176s - loss: 0.0512 - acc: 0.9814 - val_loss: 0.0509 - val_acc: 0.9815\n",
      "Epoch 7/10\n",
      "86157/86157 [==============================] - 176s - loss: 0.0497 - acc: 0.9817 - val_loss: 0.0496 - val_acc: 0.9816\n",
      "Epoch 8/10\n",
      "86157/86157 [==============================] - 183s - loss: 0.0483 - acc: 0.9821 - val_loss: 0.0490 - val_acc: 0.9818\n",
      "Epoch 9/10\n",
      "86157/86157 [==============================] - 178s - loss: 0.0478 - acc: 0.9823 - val_loss: 0.0508 - val_acc: 0.9815\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=10,\n",
    "          batch_size=hyperparam['batch_size'], \n",
    "          shuffle=True, \n",
    "          validation_split=0.1, \n",
    "          callbacks=[EarlyStopping(verbose=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('./models/Baseline_LSTM_es.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
